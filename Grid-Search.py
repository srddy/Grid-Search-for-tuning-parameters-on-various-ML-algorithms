# -*- coding: utf-8 -*-
"""Scikit-Lab-Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mpKPob7q5rNPOGwz6-TjzdU_PdQ8VAo1
"""

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.metrics import classification_report
from sklearn.model_selection import GridSearchCV
from xgboost.sklearn import XGBClassifier  
import warnings

import pandas as pd
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/tic-tac-toe/tic-tac-toe.data"
names = ['tls', 'tms', 'trs', 'mls', 'mms', 'mrs', 'bls', 'bms', 'brs', 'result']
data = pd.read_csv(url, names=names)
print(data.shape)
print(data)
data['result'] = data['result'].map({'negative': 1, 'positive': 0})
data['tls'] = data['tls'].map({'x': 3, 'o': 4, 'b': 5})
data['tms'] = data['tms'].map({'x': 3, 'o': 4, 'b': 5})
data['trs'] = data['trs'].map({'x': 3, 'o': 4, 'b': 5})
data['mls'] = data['mls'].map({'x': 3, 'o': 4, 'b': 5})
data['mms'] = data['mms'].map({'x': 3, 'o': 4, 'b': 5})
data['mrs'] = data['mrs'].map({'x': 3, 'o': 4, 'b': 5})
data['bls'] = data['bls'].map({'x': 3, 'o': 4, 'b': 5})
data['bms'] = data['bms'].map({'x': 3, 'o': 4, 'b': 5})
data['brs'] = data['brs'].map({'x': 3, 'o': 4, 'b': 5})
print(data)

x_train, x_test, y_train, y_test = train_test_split(data.loc[:, data.columns != 'result'], data.result, test_size=0.25, random_state=0)
print(x_train)
print(y_train)
print(x_test)
print(y_test)

# Commented out IPython magic to ensure Python compatibility.
scores = ['precision']
names = ['Decision Tree', 'Neural Net', 'Support Vector Machine', 'Gaussian Naive Bayes', 'Logistic Regression', 'k-Nearest Neighbors', 'Bagging', 'Random Forest', 'AdaBoost Classifier', 'Gradient Boosting Classifier', 'XGBoost']
classifiers = [DecisionTreeClassifier(), MLPClassifier(), SVC(), GaussianNB(), LogisticRegression(), KNeighborsClassifier(), BaggingClassifier(), RandomForestClassifier(), AdaBoostClassifier(), GradientBoostingClassifier(), XGBClassifier()]
warnings.filterwarnings("ignore")
for name, c in zip(names,classifiers) :
  print("Algorithm name : " + name)
  if(name == 'Decision Tree'):
    tuned_parameters = [{'max_depth': [1,2,3,4], 'min_samples_split': [2,3,4], 'min_samples_leaf': [1,2], 'min_impurity_decrease': [0.05,0.10,0.15,0.20]}]  
  if(name == 'Neural Net'):
    tuned_parameters = [{'hidden_layer_sizes': [10,25,50], 'activation': ['logistic','tanh','relu'], 'learning_rate': ['constant', 'adaptive'], 'max_iter': [5000,10000,15000]}]  
  if(name == 'Support Vector Machine'):
    tuned_parameters = [{'kernel': ['rbf','linear'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100, 1000], 'max_iter': [10,100,1000], 'random_state': [1,4,7]}]
  if(name == 'Gaussian Naive Bayes'):
    tuned_parameters = [{'priors':[None]} ]
  if(name == 'Logistic Regression'):
    tuned_parameters = [{'C': [1, 10, 100, 1000], 'fit_intercept': ['True','False'], 'max_iter': [100,200,300], 'multi_class': ['ovr','auto']}]
  if(name == 'k-Nearest Neighbors'):
    tuned_parameters = [{'n_neighbors': [5, 10, 20, 30], 'weights': ['uniform','distance'], 'algorithm': ['ball_tree','kd_tree','brute','auto'], 'p':[2,3,4]}]
  if(name == 'Bagging'):
    tuned_parameters = [{'n_estimators': [10,15,20], 'max_samples': [10,20,30], 'max_features': [3,4,5], 'random_state': [1,4,7]}]
  if(name == 'Random Forest'):
    tuned_parameters = [{'n_estimators': [10,50,100], 'criterion': ['gini','entropy'], 'max_features': [1,2,3], 'max_depth': [1,3,5]}]
  if(name == 'AdaBoost Classifier'):
    tuned_parameters = [{'n_estimators': [50,100,150], 'learning_rate': [1.0,1.5,2.0,2.5], 'algorithm': ['SAMME','SAMME.R'], 'random_state': [1,4,7]}]
  if(name == 'Gradient Boosting Classifier'):
    tuned_parameters = [{'loss': ['deviance','exponential'], 'learning_rate': [0.5,0.75,1.0],'n_estimators': [50,100,150], 'min_impurity_decrease': [0.05,0.10,0.15,0.20]}]
  if(name == 'XGBoost'):
    tuned_parameters = [{'learning_rate': [0.5,1.0,1.5], 'n_estimators': [50,100,150], 'booster': ['gbtree','gblinear','dart'], 'min_child_weight': [1,2,3]}]

  for score in scores:
      print("# Tuning hyper-parameters for %s" % score)
      print()

      clf = GridSearchCV(c, param_grid = tuned_parameters, cv=5,
                        scoring='%s_macro' % score)
      clf.fit(x_train, y_train)

      print("Best parameters set found on development set:")
      print()
      print(clf.best_params_)
      print()
      print("Grid scores on development set:")
      print()
      means = clf.cv_results_['mean_test_score']
      stds = clf.cv_results_['std_test_score']
      for mean, std, params in zip(means, stds, clf.cv_results_['params']):
          print("%0.3f (+/-%0.03f) for %r"
#                 % (mean, std * 2, params))
      print()

      print("Detailed classification report:")
      print()
      print("The model is trained on the full development set.")
      print("The scores are computed on the full evaluation set.")
      print()
      y_true, y_pred = y_test, clf.predict(x_test)
      print(classification_report(y_true, y_pred))
      print()